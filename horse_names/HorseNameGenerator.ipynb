{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\drewb\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import urllib\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.utils.class_weight\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Babbelute\n"
     ]
    }
   ],
   "source": [
    "origin_names = pd.read_csv(os.path.join('data','text.csv'))\n",
    "origin_names = np.asarray(origin_names[:]['text']) # Parse the CSV and get the column called text\n",
    "for i in range(len(origin_names)):\n",
    "    # Do our best to clean up weird characters\n",
    "    origin_names[i] = origin_names[i].encode('ascii','replace').decode('ascii').replace(u'\\x02','')\n",
    "print (origin_names[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snow White\n",
      "Mandolin\n",
      "Ash Winder\n",
      "Skippy\n",
      "Fedora\n",
      "Pardon My Dust\n",
      "Zippo's Sensation\n",
      "The Secrets Out\n",
      "Painte\n",
      "From chars to indexes: {'p': 62, 'j': 56, 'f': 52, 's': 65, 'V': 41, '\\n': 0, 'd': 50, ',': 8, '9': 17, 'D': 23, 'e': 51, 't': 66, '(': 6, '-': 9, '/': 11, \"'\": 5, '0': 12, '!': 2, '7': 15, 'w': 69, '\"': 3, 'X': 43, 'Y': 44, 'A': 20, 'F': 25, '8': 16, 'C': 22, 'O': 34, 'r': 64, '2': 13, 'I': 28, 'E': 24, 'S': 38, 'g': 53, 'W': 42, 'H': 27, 'J': 29, 'T': 39, 'o': 61, 'b': 48, 'z': 72, 'K': 30, 'L': 31, 'q': 63, '?': 19, '6': 14, '&': 4, 'i': 55, 'P': 35, 'v': 68, 'B': 21, 'N': 33, 'y': 71, ')': 7, 'h': 54, 'Q': 36, 'm': 59, 'R': 37, 'a': 47, '`': 46, 'Z': 45, 'u': 67, ' ': 1, 'l': 58, 'n': 60, 'c': 49, '.': 10, 'x': 70, 'U': 40, '=': 18, 'M': 32, 'k': 57, 'G': 26}\n",
      "From indexes to chars: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '&', 5: \"'\", 6: '(', 7: ')', 8: ',', 9: '-', 10: '.', 11: '/', 12: '0', 13: '2', 14: '6', 15: '7', 16: '8', 17: '9', 18: '=', 19: '?', 20: 'A', 21: 'B', 22: 'C', 23: 'D', 24: 'E', 25: 'F', 26: 'G', 27: 'H', 28: 'I', 29: 'J', 30: 'K', 31: 'L', 32: 'M', 33: 'N', 34: 'O', 35: 'P', 36: 'Q', 37: 'R', 38: 'S', 39: 'T', 40: 'U', 41: 'V', 42: 'W', 43: 'X', 44: 'Y', 45: 'Z', 46: '`', 47: 'a', 48: 'b', 49: 'c', 50: 'd', 51: 'e', 52: 'f', 53: 'g', 54: 'h', 55: 'i', 56: 'j', 57: 'k', 58: 'l', 59: 'm', 60: 'n', 61: 'o', 62: 'p', 63: 'q', 64: 'r', 65: 's', 66: 't', 67: 'u', 68: 'v', 69: 'w', 70: 'x', 71: 'y', 72: 'z'}\n",
      "Total num of unique char types: 73\n"
     ]
    }
   ],
   "source": [
    "num_of_names = len(origin_names)\n",
    "names = origin_names[np.random.permutation(num_of_names)]\n",
    "names = u'\\n'.join(names.tolist())\n",
    "\n",
    "# Find the unique characters in all the names\n",
    "names_chars = sorted(list(set(names)))\n",
    "\n",
    "\n",
    "chars_to_int = {}\n",
    "for i in range(len(names_chars)):\n",
    "    chars_to_int[names_chars[i]] = i\n",
    "    \n",
    "int_to_chars = {b:a for a,b in chars_to_int.items()}\n",
    "print(names[:100])\n",
    "print(\"From chars to indexes:\",chars_to_int)\n",
    "print(\"From indexes to chars:\", int_to_chars)\n",
    "print(\"Total num of unique char types:\", len(chars_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snow White\n",
      "Mandolin\n",
      "Ash Winder\n",
      "Skippy\n",
      "Fedora\n",
      "Pardo\n",
      "Example encoded:  [38, 60, 61, 69, 1, 42, 54, 55, 66, 51, 0, 32, 47, 60, 50, 61, 58, 55, 60, 0, 20, 65, 54, 1, 42, 55, 60, 50, 51, 64, 0, 38, 57, 55, 62, 62, 71, 0, 25, 51, 50, 61, 64, 47, 0, 35, 47, 64, 50, 61]\n",
      "Doc length in chars:  47910 in ints:  47910  should match.\n"
     ]
    }
   ],
   "source": [
    "# Encode all characters to their ints\n",
    "encoded = []\n",
    "for i in names:\n",
    "    encoded.append(chars_to_int[i])\n",
    "\n",
    "\n",
    "l = \"\"\n",
    "for i in encoded[:50]:\n",
    "    l += int_to_chars[i]\n",
    "\n",
    "print (l)\n",
    "print (\"Example encoded: \",encoded[:50])\n",
    "print (\"Doc length in chars: \",len(encoded), \"in ints: \", len(names), \" should match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check that each set shifts by one number per step:\n",
      "[38, 60, 61, 69, 1, 42, 54, 55, 66, 51, 0, 32, 47, 60, 50, 61, 58, 55, 60, 0, 20, 65, 54, 1, 42, 55, 60, 50, 51, 64, 0, 38, 57, 55, 62, 62, 71, 0, 25, 51] [50]\n",
      "[60, 61, 69, 1, 42, 54, 55, 66, 51, 0, 32, 47, 60, 50, 61, 58, 55, 60, 0, 20, 65, 54, 1, 42, 55, 60, 50, 51, 64, 0, 38, 57, 55, 62, 62, 71, 0, 25, 51, 50] [61]\n",
      "[61, 69, 1, 42, 54, 55, 66, 51, 0, 32, 47, 60, 50, 61, 58, 55, 60, 0, 20, 65, 54, 1, 42, 55, 60, 50, 51, 64, 0, 38, 57, 55, 62, 62, 71, 0, 25, 51, 50, 61] [64]\n",
      "Length of input and output samples should match (but number of entries in each sample don't): 47869 47869\n"
     ]
    }
   ],
   "source": [
    "window = 40 # How many characters to consider when building prediction\n",
    "\n",
    "huge_list_x = []\n",
    "huge_list_y = []\n",
    "for i in range(len(encoded) - window - 1):\n",
    "    huge_list_x.append(encoded[i:window+i])\n",
    "    huge_list_y.append([encoded[window+i]])\n",
    "    \n",
    "split = int(0.75 * len(huge_list_x))\n",
    "x_train = np.asarray(huge_list_x[:split])\n",
    "x_test = np.asarray(huge_list_x[split:])\n",
    "y_train = np.asarray(huge_list_y[:split])\n",
    "y_test = np.asarray(huge_list_y[split:])\n",
    "\n",
    "print (\"Check that each set shifts by one number per step:\")\n",
    "print(huge_list_x[0], huge_list_y[0])\n",
    "print(huge_list_x[1], huge_list_y[1])\n",
    "print(huge_list_x[2], huge_list_y[2])\n",
    "print (\"Length of input and output samples should match (but number of entries in each sample don't):\",\n",
    "       len(huge_list_x), len(huge_list_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.      ,  0.      ,  0.      , ...,  0.      ,  0.      ,\n",
       "          0.      ],\n",
       "        [ 0.02714 , -0.202417,  0.215536, ...,  0.202695, -0.032797,\n",
       "         -0.039701],\n",
       "        ...,\n",
       "        [ 0.246788, -0.299682,  0.091904, ...,  0.153139, -0.092827,\n",
       "         -0.131775],\n",
       "        [ 0.239696, -0.272422,  0.09878 , ...,  0.197519, -0.075066,\n",
       "         -0.113583],\n",
       "        [ 0.242073, -0.274988,  0.109062, ...,  0.201191, -0.06692 ,\n",
       "         -0.102592]]), 73)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From https://github.com/minimaxir/char-embeddings\n",
    "# Make a matrix that maps chars to the precomputed array of vectors\n",
    "# So a => index(54) => [0.123, -0.234, ... 287 more ..., 0.001], b = [-.999, 0.00, ... 287 more ..., 0.001], and so on.\n",
    "# Any chars not in minimaxir's precomputed list is set a value of [0.00, ... 0.000]\n",
    "char_vecs = {}\n",
    "glove_char_path = os.path.join('data','glove.840B.300d-char.txt')\n",
    "glove_char_url = 'http://raw.githubusercontent.com/minimaxir/char-embeddings/master/glove.840B.300d-char.txt'\n",
    "\n",
    "if not os.path.isfile(glove_char_path):\n",
    "    print('Downloading: ' + glove_char_url)\n",
    "    try:\n",
    "        urllib.request.urlretrieve(glove_char_url, glove_char_path)\n",
    "    except Exception as inst:\n",
    "        print(inst)\n",
    "        print('  Encountered unknown error. Continuing.')\n",
    "\n",
    "with open(glove_char_path) as f:\n",
    "    for line in f:\n",
    "        cleaned = line.strip().split(\" \")\n",
    "        char = cleaned[0]\n",
    "        vec = np.array(cleaned[1:], dtype=float)\n",
    "        char_vecs[char] = vec\n",
    "\n",
    "embedding_matrix = np.zeros((len(chars_to_int), 300))\n",
    "for char, i in chars_to_int.items():\n",
    "    embedding_vector = char_vecs.get(char)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "embedding_matrix, len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 40)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 40, 300)      21900       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, None, 256)    570368      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   (None, 256)          525312      lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 512)          131072      lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 512)          2048        dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 512)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 256)          131072      leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 256)          1024        dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 256)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 73)           18761       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 73)           18761       lstm_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "main_out (Activation)           (None, 73)           0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "aux_out (Activation)            (None, 73)           0           dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,420,318\n",
      "Trainable params: 1,396,882\n",
      "Non-trainable params: 23,436\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "chars_len = len(chars_to_int)\n",
    "\n",
    "main_input = tf.keras.Input(shape=(window,))\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    chars_len, 300, input_length=window, weights=[embedding_matrix],\n",
    "    trainable=False)# This means we don't want to make minimaxirs vocab more like our dataset (to save time)\n",
    "                    # this might be a bad assumption if your dataset isn't like MagicTheGathering cards.  \n",
    "embedded = embedding_layer(main_input)\n",
    "\n",
    "# RNN Layer\n",
    "rnn = tf.keras.layers.LSTM(256, implementation=2, return_sequences=True)(embedded)\n",
    "rnn2 = tf.keras.layers.LSTM(256, implementation=2, return_sequences=False)(rnn)\n",
    "\n",
    "aux_output = tf.keras.layers.Dense(chars_len)(rnn2)\n",
    "aux_output = tf.keras.layers.Activation('softmax', name='aux_out')(aux_output)\n",
    "\n",
    "# Hidden Layers (cargo culting a little from minimaxir, but it works)\n",
    "hidden_1 = tf.keras.layers.Dense(512, use_bias=False)(rnn2)\n",
    "hidden_1 = tf.keras.layers.BatchNormalization()(hidden_1)\n",
    "hidden_1 = tf.keras.layers.LeakyReLU()(hidden_1) # I think leakies are always just better than standard ReLUs.\n",
    "\n",
    "hidden_2 = tf.keras.layers.Dense(256, use_bias=False)(hidden_1)\n",
    "hidden_2 = tf.keras.layers.BatchNormalization()(hidden_2)\n",
    "hidden_2 = tf.keras.layers.LeakyReLU()(hidden_2)\n",
    "\n",
    "# Traditional end to a classification problem, end with number of nodes equal to possible catagories and softmax it\n",
    "main_output = tf.keras.layers.Dense(chars_len)(hidden_2)\n",
    "main_output = tf.keras.layers.Activation('softmax', name='main_out')(main_output)\n",
    "\n",
    "# Two outputs because we want to make sure the model doesn't get too far off into the weeds\n",
    "# by having it have to show immediate progress after the RNN layers, which acts to regularize \n",
    "# (keeps the model from overfitting) the model. So we're checking the model against both its\n",
    "# guess after just the RNN layers, and then after the whole model finishes.\n",
    "# Think of it as a partial credit reward system.\n",
    "model = tf.keras.Model(inputs=main_input, outputs=[main_output, aux_output])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer, loss_weights=[1., 0.2]) \n",
    "                                    # Penalize it for bad initial guesses after the RNN layers but not too much (0.2)\n",
    "                                    # vs being wrong at the end (1.0 penalty).\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One hot encoded example: [50] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1379507771522988,\n",
       " 0.256811760077256,\n",
       " 0.37484338456398264,\n",
       " 0.1620944365682087,\n",
       " 3.112623547771805,\n",
       " array([1.37950777e-01, 2.56811760e-01, 4.91794521e+02, 2.45897260e+02,\n",
       "        9.83589041e+01, 4.86925268e+00, 2.04914384e+01, 2.04914384e+01,\n",
       "        7.02563601e+01, 1.00366229e+01, 1.53685788e+01, 4.91794521e+02,\n",
       "        2.45897260e+02, 4.91794521e+02, 4.91794521e+02, 4.91794521e+02,\n",
       "        2.45897260e+02, 4.91794521e+02, 1.63931507e+02, 7.02563601e+01,\n",
       "        1.94385186e+00, 1.19367602e+00, 1.21131655e+00, 1.45501337e+00,\n",
       "        3.90313112e+00, 2.28741637e+00, 2.49641889e+00, 3.03576865e+00,\n",
       "        5.12285959e+00, 3.48790440e+00, 3.69770316e+00, 2.38735204e+00,\n",
       "        1.11014564e+00, 4.03110263e+00, 6.22524710e+00, 1.71956126e+00,\n",
       "        1.32917438e+01, 1.94385186e+00, 7.16901633e-01, 1.52258365e+00,\n",
       "        1.14370819e+01, 8.33550035e+00, 2.94487737e+00, 3.78303477e+01,\n",
       "        1.53685788e+01, 7.34021672e+00, 4.91794521e+02, 1.88571519e-01,\n",
       "        1.49028643e+00, 7.04576677e-01, 6.39524734e-01, 1.62094437e-01,\n",
       "        2.42263311e+00, 9.15818474e-01, 6.70933862e-01, 2.33188488e-01,\n",
       "        1.49028643e+01, 1.15716358e+00, 3.25045949e-01, 8.56784879e-01,\n",
       "        2.71709680e-01, 2.49641889e-01, 1.03101577e+00, 2.58839221e+01,\n",
       "        2.60071137e-01, 3.74843385e-01, 3.25691735e-01, 6.37039534e-01,\n",
       "        2.50915572e+00, 2.14757433e+00, 6.47098053e+00, 5.62050881e-01,\n",
       "        3.11262355e+00]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Everything likes to be one hot encoded:\n",
    "def quick_one_hot_encoding(items):\n",
    "    one_hotted_items = []\n",
    "    for i in items:\n",
    "        hot = np.zeros(chars_len)\n",
    "        hot[i[0]] = 1 # i is not one hot encoded as array \"hot\"\n",
    "        one_hotted_items.append(hot) # Add hot back to our encoded list\n",
    "    return np.asarray(one_hotted_items)\n",
    "\n",
    "y_train = np.asarray(huge_list_y[:split])\n",
    "y_train_hot = quick_one_hot_encoding(y_train)\n",
    "\n",
    "y_test = np.asarray(huge_list_y[split:])\n",
    "y_test_hot = quick_one_hot_encoding(y_test)\n",
    "print (\"One hot encoded example:\", y_train[0],y_train_hot[0])    \n",
    "\n",
    "# If your tet has an uneven number of characters (hint: it almost certainly does)\n",
    "# create a list of class weights to balance things out. The idea is to keep signals\n",
    "# for white-space and 'e's and 's'es in English from overpowering all the other signals\n",
    "y_labels = []\n",
    "for i in range(len(y_train)):\n",
    "    y_labels.append(y_train[i][0])\n",
    "weights = sklearn.utils.class_weight.compute_class_weight('balanced', np.unique(y_labels), y_labels)\n",
    "\n",
    "weights[chars_to_int['\\n']], weights[chars_to_int[' ']], weights[chars_to_int['s']],\\\n",
    "weights[chars_to_int['e']], weights[chars_to_int['z']], weights # Unusual letters get more weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35901 samples, validate on 11968 samples\n",
      "Epoch 1/1\n",
      "35840/35901 [============================>.]35840/35901 [============================>.] - ETA: 0s - loss: 1.4611 - main_out_loss: 1.1455 - aux_out_loss: 1.5781\n",
      "Epoch 00001: val_loss improved from inf to 1.39255, saving model to data\\text.checkpoint.h5\n",
      "35901/35901 [==============================]35901/35901 [==============================] - 24s 670us/step - loss: 1.4612 - main_out_loss: 1.1456 - aux_out_loss: 1.5781 - val_loss: 1.3926 - val_main_out_loss: 1.0817 - val_aux_out_loss: 1.5543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_checkpoint = os.path.join('data','text.checkpoint.h5')\n",
    "if os.path.isfile(data_checkpoint):\n",
    "    model.load_weights(data_checkpoint)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(data_checkpoint, save_weights_only=True, \n",
    "                                                monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n",
    "callbacks_list = [checkpoint]\n",
    "model.optimizer.lr = 0.001\n",
    "model.fit(x_train, [y_train_hot,y_train_hot], # Note the two expected outputs are the same\n",
    "          validation_data=(x_test, [y_test_hot, y_test_hot]), \n",
    "          epochs=10, batch_size=128, callbacks=callbacks_list, class_weight=[weights, weights]) # Class weights per output\n",
    "model.save_weights('text.model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From minimaxir from keras\n",
    "# adds some randomness (controlled by the temperature value 0 -> 1 log)\n",
    "# to the predicted values to keep the model from looping.\n",
    "# Taken from Boltzmann sampling  \n",
    "def textgenrnn_sample(preds, temperature):\n",
    "    '''\n",
    "    Samples predicted probabilities of the next character to allow\n",
    "    for the network to show \"creativity.\"\n",
    "    '''\n",
    "\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "\n",
    "    if temperature is None or temperature == 0.0:\n",
    "        return np.argmax(preds)\n",
    "\n",
    "    preds = np.log(preds + 1e-12) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    index = -1\n",
    "\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    index = np.argmax(probas)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ass\n",
      "Hazel\n",
      "Rebel Prades\n",
      "Hango\n",
      "Laddie\n",
      "Arthur\n",
      "Tody\n",
      "Mysty\n",
      "Santos\n",
      "Baront\n",
      "Pheance\n",
      "Riuanne\n",
      "Cappie Piquilla\n",
      "Foxy Lady\n",
      "Pilot\n",
      "Pass N Stip\n",
      "Suzie\n",
      "Chamber\n",
      "Shesa\n",
      "Watson\n",
      "Dakota Grace\n",
      "Pebasco\n",
      "Black Legend\n",
      "Apple Buise\n",
      "Painters\n",
      "Mig\n",
      "Burager\n",
      "Footsittaz Dancer\n",
      "Ragelle\n",
      "Nikita\n",
      "Grace Novelle\n",
      "Dambit\n",
      "Yokini\n",
      "Rainy\n",
      "Mander Story\n",
      "All Natural Elem\n",
      "Fox Fire\n",
      "Lisi Prince\n",
      "Pride\n",
      "Moejo the Mistmire\n",
      "Aureland Stars\n",
      "Picass Serapher\n",
      "Felicitles Advocation\n",
      "Maple Stars)\n",
      "Endles)\n",
      "Cappie\n",
      "Gem\n",
      "Pass Rose\n",
      "Pavana\n",
      "Cherokee Prize of the Wind\n",
      "Andrejan\n",
      "Divasco\n",
      "Gino\n",
      "Eagle Expectations Fiasce\n",
      "Red Sponer\n",
      "Sako\n",
      "Camuna\n",
      "Daze\n",
      "Senshi (alibue\n",
      "Star Glory\n",
      "Desert Majesty\n",
      "Cowboys Delight\n",
      "Trapper\n",
      "Rusty\n",
      "Shasta\n",
      "Acro Be Style\n",
      "My Friend\n",
      "Shanti (means Mist\n",
      "Jumper\n",
      "Murphy\n",
      "On the Rose\n",
      "Reds\n",
      "Clover Boy\n",
      "Herris\n",
      "Suble Angel\n",
      "Tempo\n",
      "Just Between Rebellion\n",
      "Ima Perdy\n",
      "Piestian Glory\n",
      "Unity\n",
      "Topaz\n",
      "Mountain\n",
      "Dimite\n",
      "Finnigan\n",
      "Red Stepper\n",
      "Trad\n",
      "Deputy\n",
      "Mica\n",
      "Mungle Commet\n",
      "Shermarkman\n",
      "Weston\n",
      "Gotta-Go (G-G)\n",
      "Lime Tate\n",
      "Court Bug\n",
      "Oscada\n",
      "Robbie\n",
      "Fizz\n",
      "Botan Heathern Wonder\n",
      "Pippy Gly Day\n",
      "Woodstock\n",
      "Conquest\n",
      "Batman\n",
      "Cuddello Yeview\n",
      "Wonder Rain\n",
      "El Percy\n",
      "Jay Trader\n",
      "Friend\n",
      "Tash\n",
      "Blue Valentine\n",
      "Fandagle\n",
      "Hanger\n",
      "Jinx\n",
      "Wild oni\n",
      "Diva\n",
      "Copy Cat\n",
      "Firenze\n",
      "Patriot\n",
      "Onjo\n",
      "Alanda\n",
      "Panoshfuchy\n",
      "Eggar A to Contric\n",
      "My Lil' Itrecks\n",
      "Abracadabra\n",
      "Winalin\n",
      "Strawberry\n",
      "Kings Jess\n",
      "Armful Overtop\n",
      "Sargent\n",
      "ReKogous\n",
      "Celtic Jewel\n",
      "Lantana Lady\n",
      "Song of the Limit\n",
      "Kitty\n",
      "Rain Dancer\n",
      "Villain\n",
      "Eura Fire\n",
      "Sher Pumpkin Export\n",
      "Stellaz\n",
      "Whisky Gen\n",
      "Ramodi\n",
      "TheTail\n",
      "True Sony\n",
      "General\n",
      "Summer Breaker\n",
      "Howy\n",
      "Fire Bo\n",
      "Archimed\n",
      "Keetawneen Wantent\n",
      "Belle\n",
      "Rex\n",
      "Rough Class\n",
      "Master Toe\n",
      "Elijah\n",
      "Orkney Jane\n",
      "Louis Magic\n",
      "Sweet Set\n",
      "Polo\n",
      "Ying Yang\n",
      "Bambi\n",
      "Xylan\n",
      "Quick Ass\n",
      "Player\n",
      "Deck Luck\n",
      "Tea\n",
      "Hunkie\n",
      "Chocolate Man Star\n",
      "Jack Frozen Affair\n",
      "Pickets\n",
      "Bliss\n",
      "Shadow Star\n",
      "Bijouette\n",
      "My Pocheco\n",
      "Nibbley\n",
      "Quizzical Risks miracles Misty)\n",
      "Sueybound\n",
      "McKenner\n",
      "Squiggy\n",
      "Rhupilver Bar\n",
      "Tootie Freedom\n",
      "Wansh Upwich\n",
      "Dormander\n",
      "Skippers Lark\n",
      "Mazzler\n",
      "Sintana Stars\n",
      "Quiz Mases\n",
      "Orcadee Streak\n",
      "Geoth Glory\n",
      "Foxtrot\n",
      "Galaxy\n",
      "Dragon\n",
      "Not Exactly Mae\n",
      "Chita\n",
      "Chamble\n",
      "Jupiter\n",
      "Skydeed\n",
      "Look At me Go\n",
      "Shesa\n",
      "Kalika\n",
      "Rose\n",
      "Gansars\n",
      "Splish-Splash\n",
      "Dusty Day Yevelle\n",
      "HollyWouty Kiss\n",
      "Sandshine\n",
      "Blazing Storm\n",
      "Obyssy\n",
      "Lexison\n",
      "Jessies Next\n",
      "Handy Apple\n",
      "Quime Drop\n",
      "Sneakers\n",
      "Milly\n",
      "Mr. Gallop\n",
      "Moonshief\n",
      "Glad Tido\n",
      "Slew Candy\n",
      "Christopher Roid\n",
      "Expecting Joy\n",
      "Shooting Sky\n",
      "Black Star\n",
      "Midnight Lady\n",
      "Magavin\n",
      "Licork\n",
      "Kipger\n",
      "Maddy\n",
      "Carsicce\n",
      "Isold Rose\n",
      "Just Bear\n",
      "Roseburg\n",
      "Renaissance\n",
      "Juno\n",
      "Satine\n",
      "Laddie\n",
      "SimplyJack\n",
      "Kept Serapher\n",
      "Skippy\n",
      "Leon\n",
      "Red One\n",
      "Shadybellie\n",
      "Sunny\n",
      "Franklina\n",
      "Zaiden\n",
      "Pride of Spice\n",
      "Toron\n",
      "Sevana\n",
      "Froven\n",
      "Zippy Knight\n",
      "Will\n",
      "Night Show\n",
      "Branze\n",
      "My Chancer\n",
      "Midnight Strikes\n",
      "Canadian Wanter\n",
      "Spencer\n",
      "Samson\n",
      "Bekimard\n",
      "Speedy Golly Maker\n",
      "Trooper\n",
      "The My I Honey\n",
      "Jack\n",
      "Stath Thing\n",
      "Belmont\n",
      "Speck of the Sky\n",
      "Yentel\n",
      "Moonbeam\n",
      "Merietta\n",
      "Smoyen\n",
      "Yuki\n",
      "Barreness\n",
      "Miss My Diddie\n",
      "Suarduster\n",
      "Ruxmont\n",
      "Wingstar\n",
      "Xanger\n",
      "Midnight Diamond\n",
      "Steppin' Star\n",
      "Dr. Pepperfusn's Dream\n",
      "Waylage\n",
      "Percy\n",
      "Paint best friend)\n",
      "Whita\n",
      "Freedoms Fire\n",
      "Gallaghern Game\n",
      "Just Berry\n",
      "Dardeon\n",
      "Donald\n",
      "Bar King\n",
      "Marshmallow Me To Than Kater\n",
      "Tomcat\n",
      "Just N Steal\n",
      "Yukon\n",
      "Clover Bar\n",
      "Desert Runner\n",
      "Celtic Popsicle\n",
      "Puzzle Days\n",
      "Sheena's Touch\n",
      "Kitty\n",
      "Taxi\n",
      "Maggie's Treasure\n",
      "Marion\n",
      "Baby G\n",
      "Requeister\n",
      "Waylan\n",
      "Hollywood Something Tune\n",
      "Zena\n",
      "Night Flight\n",
      "Glenmore Boy\n",
      "Mauharino\n",
      "Prelise\n",
      "Arigo\n",
      "Zara\n",
      "Gambi\n",
      "Marshmello\n",
      "Rain Dancer\n",
      "Keisha\n",
      "Scarlet Decoster\n",
      "Jimond Blues\n",
      "Jumpon\n",
      "White Wash\n",
      "Flame Heart\n",
      "Sagen Joy\n",
      "Polo\n",
      "Draft\n",
      "Shindeagan\n",
      "Wishedent\n",
      "Adrian\n",
      "Waylan\n",
      "Sadd\n",
      "Street Shine\n",
      "Siester\n",
      "Kaine\n",
      "Drift\n",
      "Sprite\n",
      "Shutter Beth\n",
      "Wind of Frost\n",
      "Gallagers Treasure\n",
      "Apache Cracker\n",
      "Glippers\n",
      "Tink\n",
      "Peridot\n",
      "Fireted viert\n",
      "Roxy\n",
      "Santos\n",
      "Emper\n",
      "Abaccus\n",
      "Fara\n",
      "Taydee\n",
      "Songo\n",
      "Call Medy\n",
      "Walker\n",
      "Arrow\n",
      "Juley Glory\n",
      "The Waldo\n",
      "Iben\n",
      "Wonder Storm\n",
      "Wild Man\n",
      "General Hood\n",
      "Queen of Spades\n",
      "Uly on Derry Dooby Dooby Dee Zippo\n",
      "Bumpkin\n",
      "Bazoran\n",
      "Say-so Take\n",
      "White a Dozzal\n",
      "Tess\n",
      "Uny-rort\n",
      "Jack Champarra\n",
      "Lady Hawk\n",
      "Sent Fromber\n",
      "Sierra\n",
      "Chunkarbo\n",
      "Flower's Pride\n",
      "Polar\n",
      "Master Milars\n",
      "Ritping Star\n",
      "Not A Lost\n",
      "Tommy\n",
      "Indian Paint\n",
      "Desert Angel\n",
      "Ditto\n",
      "Sunny Day\n",
      "Decker\n",
      "Little Dawn\n",
      "Giddy Eight\n",
      "Eagle Becksuff\n",
      "Sir Dark\n",
      "Infire\n",
      "Gumby\n",
      "Hawking Sensation\n",
      "Wide Above Buiscet\n",
      "Kalahather\n",
      "Mike\n",
      "Trixes\n",
      "Gringo\n",
      "Albertinal Flame\n",
      "Quicksilver Willion\n",
      "Jonathan\n",
      "Naraskan Jewel\n",
      "At Last\n",
      "Celtic Sun\n",
      "Sully\n",
      "Serenda Dancer\n",
      "Skip Peache\n",
      "Johnny\n",
      "Kelly\n",
      "Apple Butter\n",
      "Kissede\n",
      "Barsy\n",
      "Houdley\n",
      "Pendragon Jewel\n",
      "My Prince\n",
      "Shanty\n",
      "Positiver\n",
      "Strappa\n",
      "Roofus\n",
      "Edge\n",
      "Apollo's Condrall\n",
      "Doris\n",
      "Sky Dancer\n",
      "Inters\n",
      "Bilbo\n",
      "Pride\n",
      "Tody\n",
      "Incoroon\n",
      "Lucky\n",
      "Koopy Express\n",
      "Sir Prince\n",
      "Madame Buy\n",
      "Where It's At\n",
      "Southern Dancer\n",
      "Sport-& Up Jester\n",
      "Brie\n",
      "Macy\n",
      "Silver Design\n",
      "Hemlock\n",
      "Crousterour Blues\n",
      "Rustbune\n",
      "Deal Me\n",
      "Gasha\n",
      "Edwing The Check\n",
      "Logo\n",
      "Mayzie Da\n",
      "Oak\n",
      "Athena\n",
      "Yankee\n",
      "Chyla Leave aka de midled Drummer\n",
      "Tonie\n",
      "Mystical\n",
      "Edge\n",
      "Merrie\n",
      "Please Pacific\n",
      "Shining star\n",
      "Tuff Zepeal\n",
      "Pacos\n",
      "Shamin\n",
      "Starzao\n",
      "Piel\n",
      "Saturn\n",
      "Mochace\n",
      "Master Man\n",
      "Zip\n",
      "Hagene\n",
      "Feuet\n",
      "Paint best friend)\n",
      "True So Clever\n",
      "Spartanner\n",
      "Jubad\n",
      "Senshi\n",
      "Apollo\n",
      "Uakette\n",
      "Deal Me Clover\n",
      "Sweet Bandit\n",
      "Slow Shimer\n",
      "Masimo's Spot\n",
      "Double Pea\n",
      "Top Quakers crystar\n",
      "Montana\n",
      "Marshador\n",
      "Angelo\n",
      "Gold Jewel\n",
      "Moonlit\n",
      "Conquest\n",
      "Sure Blue\n",
      "Mr. Gentleman Secret\n",
      "Ghost\n",
      "Garris\n",
      "Mercy\n",
      "Chelsea\n",
      "P.S. I Love & Fort\n",
      "Bindy Rose\n",
      "Jantana Spot\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seed = x_train[102:103]\n",
    "model.predict(seed)\n",
    "text = \"\"\n",
    "i = 0\n",
    "while True:\n",
    "    # Generate the next character, pop the first char off the seed, tack on the new char to the end of the seed list, and rerun\n",
    "    # also called \"moving the window\"\n",
    "    new_char = textgenrnn_sample(model.predict(seed)[0][0], 0.95) # 0->no randomness in sampling, 1->lots of randomness\n",
    "    seed = seed[0][1:].tolist()\n",
    "    seed.append(new_char)\n",
    "    text += int_to_chars[new_char]\n",
    "    seed = np.asarray([seed])\n",
    "    i+=1\n",
    "    if i>5000 and int_to_chars[new_char]==\"\\n\": # how many characters we want to generate, \n",
    "        break                                    # and give it a chance to finish its thought\n",
    "        \n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285 491 4740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ass',\n",
       " 'Rebel Prades',\n",
       " 'Hango',\n",
       " 'Baront',\n",
       " 'Pheance',\n",
       " 'Riuanne',\n",
       " 'Cappie Piquilla',\n",
       " 'Pass N Stip',\n",
       " 'Suzie',\n",
       " 'Pebasco',\n",
       " 'Black Legend',\n",
       " 'Apple Buise',\n",
       " 'Painters',\n",
       " 'Burager',\n",
       " 'Footsittaz Dancer',\n",
       " 'Ragelle',\n",
       " 'Grace Novelle',\n",
       " 'Dambit',\n",
       " 'Yokini',\n",
       " 'Mander Story',\n",
       " 'All Natural Elem',\n",
       " 'Lisi Prince',\n",
       " 'Moejo the Mistmire',\n",
       " 'Aureland Stars',\n",
       " 'Picass Serapher',\n",
       " 'Felicitles Advocation',\n",
       " 'Maple Stars)',\n",
       " 'Endles)',\n",
       " 'Pass Rose',\n",
       " 'Pavana',\n",
       " 'Cherokee Prize of the Wind',\n",
       " 'Andrejan',\n",
       " 'Divasco',\n",
       " 'Eagle Expectations Fiasce',\n",
       " 'Red Sponer',\n",
       " 'Camuna',\n",
       " 'Senshi (alibue',\n",
       " 'Star Glory',\n",
       " 'Desert Majesty',\n",
       " 'Acro Be Style',\n",
       " 'Shanti (means Mist',\n",
       " 'On the Rose',\n",
       " 'Clover Boy',\n",
       " 'Herris',\n",
       " 'Suble Angel',\n",
       " 'Just Between Rebellion',\n",
       " 'Ima Perdy',\n",
       " 'Piestian Glory',\n",
       " 'Dimite',\n",
       " 'Red Stepper',\n",
       " 'Trad',\n",
       " 'Mungle Commet',\n",
       " 'Shermarkman',\n",
       " 'Lime Tate',\n",
       " 'Court Bug',\n",
       " 'Oscada',\n",
       " 'Botan Heathern Wonder',\n",
       " 'Pippy Gly Day',\n",
       " 'Cuddello Yeview',\n",
       " 'Wonder Rain',\n",
       " 'El Percy',\n",
       " 'Jay Trader',\n",
       " 'Friend',\n",
       " 'Fandagle',\n",
       " 'Hanger',\n",
       " 'Wild oni',\n",
       " 'Onjo',\n",
       " 'Alanda',\n",
       " 'Panoshfuchy',\n",
       " 'Eggar A to Contric',\n",
       " \"My Lil' Itrecks\",\n",
       " 'Winalin',\n",
       " 'Kings Jess',\n",
       " 'Armful Overtop',\n",
       " 'ReKogous',\n",
       " 'Song of the Limit',\n",
       " 'Eura Fire',\n",
       " 'Sher Pumpkin Export',\n",
       " 'Stellaz',\n",
       " 'TheTail',\n",
       " 'True Sony',\n",
       " 'Summer Breaker',\n",
       " 'Howy',\n",
       " 'Fire Bo',\n",
       " 'Archimed',\n",
       " 'Keetawneen Wantent',\n",
       " 'Rough Class',\n",
       " 'Master Toe',\n",
       " 'Orkney Jane',\n",
       " 'Louis Magic',\n",
       " 'Sweet Set',\n",
       " 'Xylan',\n",
       " 'Quick Ass',\n",
       " 'Deck Luck',\n",
       " 'Tea',\n",
       " 'Hunkie',\n",
       " 'Chocolate Man Star',\n",
       " 'Jack Frozen Affair',\n",
       " 'Pickets',\n",
       " 'Shadow Star',\n",
       " 'Bijouette',\n",
       " 'My Pocheco',\n",
       " 'Quizzical Risks miracles Misty)',\n",
       " 'Sueybound',\n",
       " 'McKenner',\n",
       " 'Rhupilver Bar',\n",
       " 'Tootie Freedom',\n",
       " 'Wansh Upwich',\n",
       " 'Dormander',\n",
       " 'Skippers Lark',\n",
       " 'Mazzler',\n",
       " 'Sintana Stars',\n",
       " 'Quiz Mases',\n",
       " 'Orcadee Streak',\n",
       " 'Geoth Glory',\n",
       " 'Not Exactly Mae',\n",
       " 'Chita',\n",
       " 'Chamble',\n",
       " 'Skydeed',\n",
       " 'Gansars',\n",
       " 'Dusty Day Yevelle',\n",
       " 'HollyWouty Kiss',\n",
       " 'Blazing Storm',\n",
       " 'Obyssy',\n",
       " 'Lexison',\n",
       " 'Jessies Next',\n",
       " 'Handy Apple',\n",
       " 'Quime Drop',\n",
       " 'Moonshief',\n",
       " 'Glad Tido',\n",
       " 'Slew Candy',\n",
       " 'Christopher Roid',\n",
       " 'Expecting Joy',\n",
       " 'Shooting Sky',\n",
       " 'Black Star',\n",
       " 'Licork',\n",
       " 'Kipger',\n",
       " 'Carsicce',\n",
       " 'Isold Rose',\n",
       " 'Just Bear',\n",
       " 'SimplyJack',\n",
       " 'Kept Serapher',\n",
       " 'Red One',\n",
       " 'Shadybellie',\n",
       " 'Franklina',\n",
       " 'Zaiden',\n",
       " 'Pride of Spice',\n",
       " 'Toron',\n",
       " 'Froven',\n",
       " 'Zippy Knight',\n",
       " 'Branze',\n",
       " 'My Chancer',\n",
       " 'Midnight Strikes',\n",
       " 'Canadian Wanter',\n",
       " 'Bekimard',\n",
       " 'Speedy Golly Maker',\n",
       " 'The My I Honey',\n",
       " 'Stath Thing',\n",
       " 'Speck of the Sky',\n",
       " 'Smoyen',\n",
       " 'Miss My Diddie',\n",
       " 'Suarduster',\n",
       " 'Ruxmont',\n",
       " 'Xanger',\n",
       " 'Midnight Diamond',\n",
       " \"Steppin' Star\",\n",
       " \"Dr. Pepperfusn's Dream\",\n",
       " 'Waylage',\n",
       " 'Paint best friend)',\n",
       " 'Gallaghern Game',\n",
       " 'Just Berry',\n",
       " 'Dardeon',\n",
       " 'Marshmallow Me To Than Kater',\n",
       " 'Just N Steal',\n",
       " 'Clover Bar',\n",
       " 'Desert Runner',\n",
       " 'Celtic Popsicle',\n",
       " 'Puzzle Days',\n",
       " \"Sheena's Touch\",\n",
       " \"Maggie's Treasure\",\n",
       " 'Requeister',\n",
       " 'Hollywood Something Tune',\n",
       " 'Glenmore Boy',\n",
       " 'Mauharino',\n",
       " 'Prelise',\n",
       " 'Arigo',\n",
       " 'Gambi',\n",
       " 'Scarlet Decoster',\n",
       " 'Jimond Blues',\n",
       " 'Jumpon',\n",
       " 'Flame Heart',\n",
       " 'Sagen Joy',\n",
       " 'Shindeagan',\n",
       " 'Wishedent',\n",
       " 'Sadd',\n",
       " 'Street Shine',\n",
       " 'Siester',\n",
       " 'Shutter Beth',\n",
       " 'Wind of Frost',\n",
       " 'Gallagers Treasure',\n",
       " 'Apache Cracker',\n",
       " 'Glippers',\n",
       " 'Fireted viert',\n",
       " 'Emper',\n",
       " 'Songo',\n",
       " 'Call Medy',\n",
       " 'Juley Glory',\n",
       " 'The Waldo',\n",
       " 'Wonder Storm',\n",
       " 'Wild Man',\n",
       " 'General Hood',\n",
       " 'Uly on Derry Dooby Dooby Dee Zippo',\n",
       " 'Bazoran',\n",
       " 'Say-so Take',\n",
       " 'White a Dozzal',\n",
       " 'Uny-rort',\n",
       " 'Jack Champarra',\n",
       " 'Sent Fromber',\n",
       " 'Chunkarbo',\n",
       " \"Flower's Pride\",\n",
       " 'Master Milars',\n",
       " 'Ritping Star',\n",
       " 'Not A Lost',\n",
       " 'Desert Angel',\n",
       " 'Little Dawn',\n",
       " 'Giddy Eight',\n",
       " 'Eagle Becksuff',\n",
       " 'Sir Dark',\n",
       " 'Infire',\n",
       " 'Hawking Sensation',\n",
       " 'Wide Above Buiscet',\n",
       " 'Kalahather',\n",
       " 'Trixes',\n",
       " 'Albertinal Flame',\n",
       " 'Quicksilver Willion',\n",
       " 'Naraskan Jewel',\n",
       " 'Serenda Dancer',\n",
       " 'Skip Peache',\n",
       " 'Kissede',\n",
       " 'Barsy',\n",
       " 'Houdley',\n",
       " 'Pendragon Jewel',\n",
       " 'Positiver',\n",
       " \"Apollo's Condrall\",\n",
       " 'Inters',\n",
       " 'Incoroon',\n",
       " 'Koopy Express',\n",
       " 'Madame Buy',\n",
       " 'Southern Dancer',\n",
       " 'Sport-& Up Jester',\n",
       " 'Silver Design',\n",
       " 'Crousterour Blues',\n",
       " 'Rustbune',\n",
       " 'Deal Me',\n",
       " 'Gasha',\n",
       " 'Edwing The Check',\n",
       " 'Mayzie Da',\n",
       " 'Chyla Leave aka de midled Drummer',\n",
       " 'Tonie',\n",
       " 'Please Pacific',\n",
       " 'Tuff Zepeal',\n",
       " 'Piel',\n",
       " 'Mochace',\n",
       " 'Master Man',\n",
       " 'Hagene',\n",
       " 'Feuet',\n",
       " 'Paint best friend)',\n",
       " 'True So Clever',\n",
       " 'Spartanner',\n",
       " 'Jubad',\n",
       " 'Uakette',\n",
       " 'Deal Me Clover',\n",
       " 'Sweet Bandit',\n",
       " 'Slow Shimer',\n",
       " \"Masimo's Spot\",\n",
       " 'Double Pea',\n",
       " 'Top Quakers crystar',\n",
       " 'Marshador',\n",
       " 'Moonlit',\n",
       " 'Sure Blue',\n",
       " 'Mr. Gentleman Secret',\n",
       " 'P.S. I Love & Fort',\n",
       " 'Bindy Rose',\n",
       " 'Jantana Spot',\n",
       " '']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "new_names = text.split(\"\\n\")\n",
    "originals = names.split('\\n')\n",
    "for i in new_names: # Remove duplicates (you'll get a lot)\n",
    "    if i not in originals:\n",
    "        results.append(i)\n",
    "print (len(results), len(new_names), len(np.unique(originals)))\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
